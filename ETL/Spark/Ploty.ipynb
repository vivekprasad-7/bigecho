{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import concat \n",
    "# from pyspark.sql.functions import when \n",
    "# from pyspark.sql.functions import lit \n",
    "# from pyspark.sql.functions import col\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Column\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = SparkSession.builder.master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframereader = session.read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataframereader.csv(\"mycsv1.csv\",header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= df.withColumnRenamed(\"_c0\",\"Skey\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|Probability|\n",
      "+-----------+\n",
      "|beta|      |\n",
      "|alpha|     |\n",
      "+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"Proba\",lit(\"\"))\n",
    "\n",
    "df.withColumn(\"Probability\",concat(\"Proba\",\n",
    "when(df.max_consec_weeks>1,\"alpha|\").otherwise(\"beta|\")   \n",
    ")).select(\"Probability\").show(2,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+---------+---------+\n",
      "|CustomerID|weeks_shopped|total_qty|med_spend|\n",
      "+----------+-------------+---------+---------+\n",
      "|     12540|           16|    12540|      0.0|\n",
      "|     12665|            1|    12665|  3962.85|\n",
      "|     12897|            2|    12897|   329.52|\n",
      "|     13476|            2|    13476|  1680.88|\n",
      "|     16232|            3|    16232|    793.5|\n",
      "|     16352|            2|    16352|  4050.61|\n",
      "+----------+-------------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#df.filter(col(\"distinct_periods\")==(1)).show()\n",
    "df1= df.select(\"CustomerID\",\"weeks_shopped\")\n",
    "df2= df.select(\"total_qty\",\"med_spend\")\n",
    "#cond = Column(df1.CustomerID==df2.total_qty)\n",
    "df1.join(df2,df1.CustomerID==df2.total_qty,\"inner\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_mysql = session.read\\\n",
    "    .format(\"jdbc\")\\\n",
    "    .option(\"url\", \"jdbc:mysql://localhost/arms\")\\\n",
    "    .option(\"driver\", \"com.mysql.jdbc.Driver\")\\\n",
    "    .option(\"dbtable\", \"users\").option(\"user\", \"root\")\\\n",
    "    .option(\"password\", \"Programming1!\").load()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+----------+---------+--------------------+------------+-------------------+--------------+\n",
      "| username|            password|first_name|Last_name|               email|phone_number|         last_login|       api_key|\n",
      "+---------+--------------------+----------+---------+--------------------+------------+-------------------+--------------+\n",
      "|amansingh|3192b26ef8d0595ba...|      Aman|    Singh|singhaman11415@gm...|  8770262013|2020-06-06 18:49:41|abhikuchnhihai|\n",
      "|     root|              dummy1|         v|   prasad|   vprasad@gmail.com|          99|2020-06-07 14:19:17|         nulll|\n",
      "|  vprasad|               dummy|         v|   prasad|   vprasad@gmail.com|          99|2020-07-05 20:59:30|         nulll|\n",
      "+---------+--------------------+----------+---------+--------------------+------------+-------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataframe_mysql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlData = [(\"first\",\"123\",\"spark\",\"sql\",\"sparkk\",999,\"2020-06-06 18:49:41\",\"\")]\n",
    "sqldf = session.createDataFrame(sqlData,[\"username\",\"password\",\"first_name\",\"Last_name\",\"email\",\"phone_number\",\"last_login\",\"api_key\"])\n",
    "sqldf.write.format('jdbc').options(\n",
    "      url='jdbc:mysql://localhost/arms',\n",
    "      driver='com.mysql.jdbc.Driver',\n",
    "      dbtable='users',\n",
    "      user='root',\n",
    "      password='Programming1!').mode('append').save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+--------+\n",
      "|Verified| username|Starting|\n",
      "+--------+---------+--------+\n",
      "|    True|amansingh|   false|\n",
      "|    True|    first|   false|\n",
      "|    True|     root|   false|\n",
      "|   false|  vprasad|    True|\n",
      "+--------+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chcekOperator = dataframe_mysql.withColumn(\"Verified\",when(dataframe_mysql.username!=\"vprasad\",lit(\"True\"))\n",
    "                                           .otherwise(lit(\"false\")))\\\n",
    "                                .withColumn(\"Starting\",when(dataframe_mysql.username.startswith(\"vp\"),lit(\"True\"))#startswith\n",
    "                                           .otherwise(lit(\"false\")))\n",
    "chcekOperator.select(\"Verified\",\"username\",\"Starting\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+------+----------+-------------+\n",
      "|skey|CustomerID|shabit|spend_desc|visit_desc   |\n",
      "+----+----------+------+----------+-------------+\n",
      "|0   |12346     |PO    |1.High    |6.Hardly Ever|\n",
      "|1   |12347     |PR    |1.High    |1.Daily      |\n",
      "|2   |12348     |PO    |2.Medium  |5.Now & Then |\n",
      "|3   |12349     |PO    |1.High    |6.Hardly Ever|\n",
      "|4   |12350     |UN    |2.Medium  |6.Hardly Ever|\n",
      "|5   |12352     |VL    |2.Medium  |1.Daily      |\n",
      "|6   |12353     |UN    |3.Low     |6.Hardly Ever|\n",
      "|7   |12354     |PO    |1.High    |6.Hardly Ever|\n",
      "|8   |12355     |UN    |2.Medium  |6.Hardly Ever|\n",
      "|9   |12356     |UN    |2.Medium  |6.Hardly Ever|\n",
      "+----+----------+------+----------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"skey\",\"CustomerID\",\"shabit\",\"spend_desc\",\"visit_desc\").show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+\n",
      "|shabit|custCount|\n",
      "+------+---------+\n",
      "|    VL|      445|\n",
      "|    PO|     1061|\n",
      "|    UN|     2697|\n",
      "|    PR|      136|\n",
      "+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#query1-> find customer in each shabit\n",
    "'''rnkdf = df.withColumn(\"rankpk\",F.dense_rank().over(Window.partitionBy(\"CustomerID\").orderBy(\"CustomerID\"))) \\\n",
    ".select(\"rankpk\",\"CustomerID\").filter(\"rankpk>1\").show(10,False)'''\n",
    "df.groupBy('shabit').count().withColumnRenamed(\"count\",\"custCount\").filter(\"custCount>1\") \\\n",
    ".show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframereader1 = session.read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+---------+\n",
      "|docType|supplyType|InvoiceID|\n",
      "+-------+----------+---------+\n",
      "|INV    |TAX       |9812INV  |\n",
      "|INV    |TAX       |9812INV  |\n",
      "|ABC    |TAX       |9812INV  |\n",
      "|INV    |TAX       |9813INV  |\n",
      "|INV    |INV       |9813INV  |\n",
      "|INV    |NON       |9814INV  |\n",
      "|INV    |TAX       |9814INV  |\n",
      "|INV    |TAX       |9814INV  |\n",
      "+-------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfOutward = dataframereader1.csv(\"Outward.csv\",header=True)\n",
    "dfOutward.show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+---------+----------+---+\n",
      "|docType|supplyType|InvoiceID|InvoiceIDD|BR2|\n",
      "+-------+----------+---------+----------+---+\n",
      "|    INV|       TAX|  9812INV|      null|  2|\n",
      "|    INV|       TAX|  9812INV|      null|  2|\n",
      "|    ABC|       TAX|  9812INV|      null|  2|\n",
      "|    INV|       TAX|  9813INV|   9813INV|  2|\n",
      "|    INV|       INV|  9813INV|   9813INV|  1|\n",
      "|    INV|       NON|  9814INV|   9814INV|  2|\n",
      "|    INV|       TAX|  9814INV|   9814INV|  2|\n",
      "|    INV|       TAX|  9814INV|   9814INV|  2|\n",
      "+-------+----------+---------+----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''dfOutward.groupBy([\"InvoiceID\"]).count(\"supplyType\").withColumnRenamed(\"count\",\"mulSupply\")\\\n",
    ".show(20,False)'''\n",
    "\n",
    "w = Window.partitionBy(\"InvoiceID\").orderBy(\"supplyType\")\n",
    "\n",
    "dupInvoice = dfOutward.withColumn(\"srank\",F.dense_rank().over(w)) \\\n",
    ".filter(\"srank>1\").select(\"InvoiceID\").withColumnRenamed(\"InvoiceID\",\"InvoiceIDD\").dropDuplicates()\n",
    "\n",
    "on=dfOutward.InvoiceID==dupInvoice.InvoiceIDD\n",
    "\n",
    "withNull = dfOutward.join(dupInvoice,on,\"leftouter\")\n",
    "withNull.withColumn(\"BR2\", F.when(withNull.InvoiceIDD is not None and withNull.supplyType!=\"INV\" ,F.lit(\"2\") ).otherwise(\"1\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method dropDuplicates in module pyspark.sql.dataframe:\n",
      "\n",
      "dropDuplicates(subset=None) method of pyspark.sql.dataframe.DataFrame instance\n",
      "    Return a new :class:`DataFrame` with duplicate rows removed,\n",
      "    optionally only considering certain columns.\n",
      "    \n",
      "    For a static batch :class:`DataFrame`, it just drops duplicate rows. For a streaming\n",
      "    :class:`DataFrame`, it will keep all data across triggers as intermediate state to drop\n",
      "    duplicates rows. You can use :func:`withWatermark` to limit how late the duplicate data can\n",
      "    be and system will accordingly limit the state. In addition, too late data older than\n",
      "    watermark will be dropped to avoid any possibility of duplicates.\n",
      "    \n",
      "    :func:`drop_duplicates` is an alias for :func:`dropDuplicates`.\n",
      "    \n",
      "    >>> from pyspark.sql import Row\n",
      "    >>> df = sc.parallelize([ \\\n",
      "    ...     Row(name='Alice', age=5, height=80), \\\n",
      "    ...     Row(name='Alice', age=5, height=80), \\\n",
      "    ...     Row(name='Alice', age=10, height=80)]).toDF()\n",
      "    >>> df.dropDuplicates().show()\n",
      "    +---+------+-----+\n",
      "    |age|height| name|\n",
      "    +---+------+-----+\n",
      "    |  5|    80|Alice|\n",
      "    | 10|    80|Alice|\n",
      "    +---+------+-----+\n",
      "    \n",
      "    >>> df.dropDuplicates(['name', 'height']).show()\n",
      "    +---+------+-----+\n",
      "    |age|height| name|\n",
      "    +---+------+-----+\n",
      "    |  5|    80|Alice|\n",
      "    +---+------+-----+\n",
      "    \n",
      "    .. versionadded:: 1.4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(dfOutward.dropDuplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install --yes plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
